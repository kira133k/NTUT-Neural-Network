# -*- coding: utf-8 -*-
"""Homework01-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13hW9_oo6IQ6QoNitqqp696hUmNWqfsO2
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score

r = 10
w = 6
n_train = 1000
n_test = 2000
n_epochs = 50

def generate_double_moon(samples, d, r=10, w=6):
    radii_upper = np.random.uniform(low=r, high=r + w, size=samples)
    thetas_upper = np.random.uniform(low=0, high=np.pi, size=samples)
    x_upper = radii_upper * np.cos(thetas_upper)
    y_upper = radii_upper * np.sin(thetas_upper)

    radii_lower = np.random.uniform(low=r, high=r + w, size=samples)
    thetas_lower = np.random.uniform(low=np.pi, high=2 * np.pi, size=samples)
    x_lower = radii_lower * np.cos(thetas_lower) + r
    y_lower = radii_lower * np.sin(thetas_lower) - d

    X = np.vstack([np.column_stack([x_upper, y_upper]), np.column_stack([x_lower, y_lower])])
    Y = np.hstack([np.ones(samples), -np.ones(samples)])
    return X, Y

def plot_decision_boundary(X, Y, model, title):
    plt.figure(figsize=(8, 6))
    plt.scatter(X[Y == 1, 0], X[Y == 1, 1], c='r', marker='o', label='Region A', alpha=0.7)
    plt.scatter(X[Y == -1, 0], X[Y == -1, 1], c='black', marker='x', label='Region B', alpha=0.7)

    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx = np.linspace(xlim[0], xlim[1], 30)
    yy = np.linspace(ylim[0], ylim[1], 30)
    XX, YY = np.meshgrid(xx, yy)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T

    try:
        Z = model.decision_function(xy).reshape(XX.shape)
        ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])
    except AttributeError:
        pass

    plt.title(title)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.axis('equal')

d = 1
learning_rate = 1e-1

print(f"\n{'='*50}")
print(f"執行實驗: d = {d}, learning rate = {learning_rate}")
print(f"{'='*50}")

X_train, Y_train = generate_double_moon(n_train, d, r, w)
X_test, Y_test = generate_double_moon(n_test, d, r, w)

model = Perceptron(max_iter=1, eta0=learning_rate, random_state=42, warm_start=True, tol=None)
model.coef_ = np.zeros((1, X_train.shape[1]))
model.intercept_ = np.zeros(1)
model.classes_ = np.array([-1, 1])

error_history = []
mse_history = []

Y_pred_init = model.predict(X_train)
error_history.append(np.mean(Y_pred_init != Y_train))
mse_history.append(np.mean((Y_pred_init - Y_train) ** 2))

for epoch in range(1, n_epochs + 1):
    model.fit(X_train, Y_train)

    Y_pred_train = model.predict(X_train)
    error_history.append(np.mean(Y_pred_train != Y_train))
    mse_history.append(np.mean((Y_pred_train - Y_train) ** 2))

Y_pred = model.predict(X_test)
num_errors = np.sum(Y_pred != Y_test)
total_samples = len(Y_test)
test_error_rate = (num_errors / total_samples) * 100

print(f"Misclassification points: {num_errors}")
print(f"Total Test points: {total_samples}")
print(f"Misclassification rate: {test_error_rate:.1f}%")
plot_title = f'Classification with d = {d}, learning rate = {learning_rate}'
plot_decision_boundary(X_train, Y_train, model, plot_title)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(range(0, n_epochs + 1), mse_history, linewidth=2, color='blue')
plt.xlabel("Number of epochs", fontsize=12)
plt.ylabel("MSE", fontsize=12)
plt.title(f"Learning Curve (d = {d}, learning_rate = {learning_rate})", fontsize=14, fontweight='bold')
plt.grid(True, which="both", linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()